{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import nltk\r\n",
    "import ssl\r\n",
    "try:\r\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\r\n",
    "except AttributeError:\r\n",
    "    pass\r\n",
    "else:\r\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\r\n",
    "\r\n",
    "nltk.download('punkt')\r\n",
    "nltk.download('wordnet')  \r\n",
    "from nltk.stem import WordNetLemmatizer\r\n",
    "lemmatizer = WordNetLemmatizer()\r\n",
    "import json\r\n",
    "import pickle\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "from tensorflow.keras.models import Sequential\r\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\r\n",
    "from tensorflow.keras.optimizers import SGD\r\n",
    "import random\r\n",
    "\r\n",
    "words=[]\r\n",
    "classes = []\r\n",
    "documents = []\r\n",
    "ignore_words = ['?', '!']\r\n",
    "data_file = open('./data/data.json').read()\r\n",
    "intents = json.loads(data_file)\r\n",
    "\r\n",
    "\r\n",
    "for intent in intents['intents']:\r\n",
    "    for pattern in intent['patterns']:\r\n",
    "\r\n",
    "        # pattern에 존재하는 문장의 각 단어들을 tokenize\r\n",
    "        w = nltk.word_tokenize(pattern)\r\n",
    "        words.extend(w)\r\n",
    "\r\n",
    "        # add documents in the corpus\r\n",
    "        documents.append((w, intent['tag']))\r\n",
    "\r\n",
    "        # add to our classes list\r\n",
    "        if intent['tag'] not in classes:\r\n",
    "            classes.append(intent['tag'])\r\n",
    "\r\n",
    "# lemmaztize and lower each word and remove duplicates\r\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\r\n",
    "words = sorted(list(set(words)))\r\n",
    "\r\n",
    "# sort classes\r\n",
    "classes = sorted(list(set(classes)))\r\n",
    "\r\n",
    "# documents = combination between patterns and intents\r\n",
    "print(len(documents), \"documents\")\r\n",
    "\r\n",
    "# classes = intents\r\n",
    "print(len(classes), \"classes\", classes)\r\n",
    "\r\n",
    "# words = all words, vocabulary\r\n",
    "print(len(words), \"unique lemmatized words\", words)\r\n",
    "\r\n",
    "\r\n",
    "pickle.dump(words,open('texts.pkl','wb'))\r\n",
    "pickle.dump(classes,open('labels.pkl','wb'))\r\n",
    "\r\n",
    "# create our training data\r\n",
    "training = []\r\n",
    "# create an empty array for our output\r\n",
    "output_empty = [0] * len(classes)\r\n",
    "\r\n",
    "# training set, bag of words for each sentence\r\n",
    "for doc in documents:\r\n",
    "    # initialize our bag of words\r\n",
    "    bag = []\r\n",
    "    # list of tokenized words for the pattern\r\n",
    "    pattern_words = doc[0]\r\n",
    "    # lemmatize each word - create base word, in attempt to represent related words\r\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\r\n",
    "    # create our bag of words array with 1, if word match found in current pattern\r\n",
    "    for w in words:\r\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\r\n",
    "    \r\n",
    "    # output is a '0' for each tag and '1' for current tag (for each pattern)\r\n",
    "    output_row = list(output_empty)\r\n",
    "    output_row[classes.index(doc[1])] = 1\r\n",
    "    \r\n",
    "    training.append([bag, output_row])\r\n",
    "\r\n",
    "# shuffle our features and turn into np.array\r\n",
    "random.shuffle(training)\r\n",
    "training = np.array(training)\r\n",
    "\r\n",
    "# create train and test lists. X - patterns, Y - intents\r\n",
    "train_x = list(training[:,0])\r\n",
    "train_y = list(training[:,1])\r\n",
    "print(\"Training data created\")\r\n",
    "\r\n",
    "\r\n",
    "# Create model - 3 layers. First layer 128 neurons, second layer 64 neurons and 3rd output layer contains number of neurons\r\n",
    "# equal to number of intents to predict output intent with softmax\r\n",
    "model = Sequential()\r\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\r\n",
    "model.add(Dropout(0.5))\r\n",
    "model.add(Dense(64, activation='relu'))\r\n",
    "model.add(Dropout(0.5))\r\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\r\n",
    "\r\n",
    "# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\r\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\r\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\r\n",
    "\r\n",
    "#fitting and saving the model \r\n",
    "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\r\n",
    "model.save('model.h5', hist)\r\n",
    "\r\n",
    "print(\"model created\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "47 documents\n",
      "9 classes ['adverse_drug', 'blood_pressure', 'blood_pressure_search', 'goodbye', 'greeting', 'hospital_search', 'options', 'pharmacy_search', 'thanks']\n",
      "88 unique lemmatized words [\"'s\", ',', 'a', 'adverse', 'all', 'anyone', 'are', 'awesome', 'be', 'behavior', 'blood', 'by', 'bye', 'can', 'causing', 'chatting', 'check', 'could', 'data', 'day', 'detail', 'do', 'dont', 'drug', 'entry', 'find', 'for', 'give', 'good', 'goodbye', 'have', 'hello', 'help', 'helpful', 'helping', 'hey', 'hi', 'history', 'hola', 'hospital', 'how', 'i', 'id', 'is', 'later', 'list', 'load', 'locate', 'log', 'looking', 'lookup', 'management', 'me', 'module', 'nearby', 'next', 'nice', 'of', 'offered', 'open', 'patient', 'pharmacy', 'pressure', 'provide', 'reaction', 'related', 'result', 'search', 'searching', 'see', 'show', 'suitable', 'support', 'task', 'thank', 'thanks', 'that', 'there', 'till', 'time', 'to', 'transfer', 'up', 'want', 'what', 'which', 'with', 'you']\n",
      "Training data created\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-4-d00503b864cc>:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  training = np.array(training)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/200\n",
      "10/10 [==============================] - 1s 2ms/step - loss: 2.2071 - accuracy: 0.1832\n",
      "Epoch 2/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 2.1531 - accuracy: 0.1258\n",
      "Epoch 3/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 2.1545 - accuracy: 0.2685\n",
      "Epoch 4/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 1.9184 - accuracy: 0.3156\n",
      "Epoch 5/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 1.8453 - accuracy: 0.4758\n",
      "Epoch 6/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 1.7747 - accuracy: 0.4737\n",
      "Epoch 7/200\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 1.5018 - accuracy: 0.5913\n",
      "Epoch 8/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 1.4745 - accuracy: 0.5250\n",
      "Epoch 9/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 1.4644 - accuracy: 0.5888\n",
      "Epoch 10/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 1.1727 - accuracy: 0.7253\n",
      "Epoch 11/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 1.2519 - accuracy: 0.6378\n",
      "Epoch 12/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 1.1015 - accuracy: 0.7186\n",
      "Epoch 13/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 1.2171 - accuracy: 0.6212\n",
      "Epoch 14/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.9481 - accuracy: 0.7404\n",
      "Epoch 15/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.7388 - accuracy: 0.8257\n",
      "Epoch 16/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.6208 - accuracy: 0.8165\n",
      "Epoch 17/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.6860 - accuracy: 0.7812\n",
      "Epoch 18/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.6910 - accuracy: 0.7167\n",
      "Epoch 19/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.4697 - accuracy: 0.9200\n",
      "Epoch 20/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.6001 - accuracy: 0.8556\n",
      "Epoch 21/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.4105 - accuracy: 0.8893\n",
      "Epoch 22/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3687 - accuracy: 0.9301\n",
      "Epoch 23/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3837 - accuracy: 0.8912\n",
      "Epoch 24/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3786 - accuracy: 0.9089\n",
      "Epoch 25/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.4202 - accuracy: 0.7968\n",
      "Epoch 26/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3523 - accuracy: 0.9098\n",
      "Epoch 27/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.2724 - accuracy: 0.9214\n",
      "Epoch 28/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3510 - accuracy: 0.8558\n",
      "Epoch 29/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1555 - accuracy: 0.9593\n",
      "Epoch 30/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.2393 - accuracy: 0.9780\n",
      "Epoch 31/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1478 - accuracy: 0.9961\n",
      "Epoch 32/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.4822 - accuracy: 0.9171\n",
      "Epoch 33/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.2787 - accuracy: 0.9120\n",
      "Epoch 34/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1893 - accuracy: 0.9826\n",
      "Epoch 35/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.2201 - accuracy: 0.9339\n",
      "Epoch 36/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1292 - accuracy: 0.9629\n",
      "Epoch 37/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1962 - accuracy: 0.9826\n",
      "Epoch 38/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.2171 - accuracy: 0.9651\n",
      "Epoch 39/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.2039 - accuracy: 0.8795\n",
      "Epoch 40/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1448 - accuracy: 0.9755\n",
      "Epoch 41/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3099 - accuracy: 0.9087\n",
      "Epoch 42/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0981 - accuracy: 1.0000\n",
      "Epoch 43/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.2268 - accuracy: 0.9150\n",
      "Epoch 44/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1073 - accuracy: 0.9826\n",
      "Epoch 45/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0817 - accuracy: 1.0000\n",
      "Epoch 46/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0649 - accuracy: 1.0000\n",
      "Epoch 47/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1751 - accuracy: 0.9185\n",
      "Epoch 48/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1232 - accuracy: 1.0000\n",
      "Epoch 49/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0453 - accuracy: 1.0000\n",
      "Epoch 50/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0962 - accuracy: 1.0000\n",
      "Epoch 51/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1011 - accuracy: 0.9696\n",
      "Epoch 52/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0836 - accuracy: 0.9582\n",
      "Epoch 53/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0803 - accuracy: 0.9941\n",
      "Epoch 54/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0778 - accuracy: 0.9862\n",
      "Epoch 55/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1667 - accuracy: 0.9780\n",
      "Epoch 56/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0475 - accuracy: 0.9892\n",
      "Epoch 57/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0761 - accuracy: 0.9629\n",
      "Epoch 58/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0290 - accuracy: 1.0000\n",
      "Epoch 59/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0527 - accuracy: 0.9892\n",
      "Epoch 60/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0590 - accuracy: 0.9826\n",
      "Epoch 61/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1426 - accuracy: 0.9582\n",
      "Epoch 62/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1671 - accuracy: 0.9593\n",
      "Epoch 63/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0485 - accuracy: 1.0000\n",
      "Epoch 64/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0506 - accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.1793 - accuracy: 0.9780\n",
      "Epoch 66/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0485 - accuracy: 0.9862\n",
      "Epoch 67/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0382 - accuracy: 1.0000\n",
      "Epoch 68/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.1445 - accuracy: 0.9439\n",
      "Epoch 69/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0809 - accuracy: 1.0000\n",
      "Epoch 70/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0672 - accuracy: 0.9918\n",
      "Epoch 71/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0404 - accuracy: 0.9941\n",
      "Epoch 72/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0400 - accuracy: 0.9859\n",
      "Epoch 73/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0574 - accuracy: 0.9862\n",
      "Epoch 74/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0181 - accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1028 - accuracy: 0.9521\n",
      "Epoch 76/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0322 - accuracy: 0.9892\n",
      "Epoch 77/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0471 - accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0643 - accuracy: 0.9780\n",
      "Epoch 80/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0409 - accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0289 - accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0412 - accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0284 - accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.1121 - accuracy: 0.9447\n",
      "Epoch 85/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0243 - accuracy: 0.9941\n",
      "Epoch 86/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0242 - accuracy: 1.0000\n",
      "Epoch 87/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0524 - accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0167 - accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0525 - accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0179 - accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0359 - accuracy: 0.9892\n",
      "Epoch 92/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0676 - accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0645 - accuracy: 0.9629\n",
      "Epoch 94/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0559 - accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.2148 - accuracy: 0.9826\n",
      "Epoch 96/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0255 - accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0395 - accuracy: 0.9862\n",
      "Epoch 99/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0230 - accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0426 - accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0805 - accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0690 - accuracy: 0.9918\n",
      "Epoch 103/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0887 - accuracy: 0.9720\n",
      "Epoch 105/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0169 - accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1594 - accuracy: 0.9068\n",
      "Epoch 107/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0246 - accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0695 - accuracy: 0.9892\n",
      "Epoch 109/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0437 - accuracy: 0.9941\n",
      "Epoch 110/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0206 - accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0225 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0377 - accuracy: 0.9862\n",
      "Epoch 113/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1031 - accuracy: 0.9629\n",
      "Epoch 114/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0180 - accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0240 - accuracy: 0.9961\n",
      "Epoch 117/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0495 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0408 - accuracy: 0.9892\n",
      "Epoch 119/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0349 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0181 - accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0950 - accuracy: 0.9447\n",
      "Epoch 124/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0334 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0168 - accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0203 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0446 - accuracy: 0.9862\n",
      "Epoch 128/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0279 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0197 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0578 - accuracy: 0.9862\n",
      "Epoch 131/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0283 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0402 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0162 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0390 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0236 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0371 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0367 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0166 - accuracy: 0.9961\n",
      "Epoch 143/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0154 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0177 - accuracy: 0.9918\n",
      "Epoch 145/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0178 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0378 - accuracy: 0.9941\n",
      "Epoch 147/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0138 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0350 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0515 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0163 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0368 - accuracy: 0.9720\n",
      "Epoch 152/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0343 - accuracy: 0.9918\n",
      "Epoch 153/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0158 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0794 - accuracy: 0.9447\n",
      "Epoch 156/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0576 - accuracy: 0.9862\n",
      "Epoch 157/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0117 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0634 - accuracy: 0.9720\n",
      "Epoch 159/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0181 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0382 - accuracy: 0.9862\n",
      "Epoch 161/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0231 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0159 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0426 - accuracy: 0.9826\n",
      "Epoch 164/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1762 - accuracy: 0.9447\n",
      "Epoch 165/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0459 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0181 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0998 - accuracy: 0.9455\n",
      "Epoch 172/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0560 - accuracy: 0.9447\n",
      "Epoch 175/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0300 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0240 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0272 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0712 - accuracy: 0.9447\n",
      "Epoch 179/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0373 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0210 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0087 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0298 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0200 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0105 - accuracy: 0.9918\n",
      "Epoch 191/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0223 - accuracy: 0.9892\n",
      "Epoch 195/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0197 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0203 - accuracy: 0.9941\n",
      "Epoch 199/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0098 - accuracy: 1.0000\n",
      "model created\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from datetime import date, datetime\r\n",
    "import nltk\r\n",
    "import ssl\r\n",
    "try:\r\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\r\n",
    "except AttributeError:\r\n",
    "    pass\r\n",
    "else:\r\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\r\n",
    "\r\n",
    "nltk.download('punkt')\r\n",
    "nltk.download('popular')\r\n",
    "\r\n",
    "from nltk.stem import WordNetLemmatizer\r\n",
    "lemmatizer = WordNetLemmatizer()\r\n",
    "import pickle\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "from tensorflow.keras.models import load_model\r\n",
    "model = load_model('model.h5')\r\n",
    "import json\r\n",
    "import random\r\n",
    "intents = json.loads(open('./data/data.json').read())\r\n",
    "words = pickle.load(open('texts.pkl','rb'))\r\n",
    "classes = pickle.load(open('labels.pkl','rb'))\r\n",
    "\r\n",
    "def clean_up_sentence(sentence):\r\n",
    "    # tokenize the pattern - split words into array\r\n",
    "    sentence_words = nltk.word_tokenize(sentence)\r\n",
    "    # stem each word - create short form for word\r\n",
    "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\r\n",
    "    return sentence_words\r\n",
    "\r\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\r\n",
    "\r\n",
    "def bow(sentence, words, show_details=True):\r\n",
    "    # tokenize the pattern\r\n",
    "    sentence_words = clean_up_sentence(sentence)\r\n",
    "    # bag of words - matrix of N words, vocabulary matrix\r\n",
    "    bag = [0]*len(words)  \r\n",
    "    for s in sentence_words:\r\n",
    "        for i,w in enumerate(words):\r\n",
    "            if w == s: \r\n",
    "                # assign 1 if current word is in the vocabulary position\r\n",
    "                bag[i] = 1\r\n",
    "                if show_details:\r\n",
    "                    print (\"found in bag: %s\" % w)\r\n",
    "    return(np.array(bag))\r\n",
    "\r\n",
    "def predict_class(sentence, model):\r\n",
    "    # filter out predictions below a threshold\r\n",
    "    p = bow(sentence, words,show_details=False)\r\n",
    "    res = model.predict(np.array([p]))[0]\r\n",
    "    ERROR_THRESHOLD = 0.25\r\n",
    "    results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\r\n",
    "    # sort by strength of probability\r\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\r\n",
    "    return_list = []\r\n",
    "    for r in results:\r\n",
    "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\r\n",
    "    return return_list\r\n",
    "\r\n",
    "def getResponse(ints, intents_json):\r\n",
    "    tag = ints[0]['intent']\r\n",
    "    list_of_intents = intents_json['intents']\r\n",
    "    for i in list_of_intents:\r\n",
    "        if(i['tag']== tag):\r\n",
    "            result = random.choice(i['responses'])\r\n",
    "            break\r\n",
    "    return result\r\n",
    "\r\n",
    "def chatbot_response(msg):\r\n",
    "    ints = predict_class(msg, model)\r\n",
    "    res = getResponse(ints, intents)\r\n",
    "    return res\r\n",
    "\r\n",
    "\r\n",
    "from flask import Flask, render_template, request\r\n",
    "from flask_ngrok import run_with_ngrok\r\n",
    "\r\n",
    "app = Flask(__name__)\r\n",
    "run_with_ngrok(app)\r\n",
    "app.static_folder = 'static'\r\n",
    "\r\n",
    "@app.route(\"/\")\r\n",
    "def home():\r\n",
    "    print(datetime.now().hour)\r\n",
    "    if len(str(datetime.now().hour))==1:\r\n",
    "        if len(str(datetime.now().minute))==1:\r\n",
    "            time = '0'+str(datetime.now().hour)+':'+'0'+str(datetime.now().minute)\r\n",
    "        else:\r\n",
    "            time = '0'+str(datetime.now().hour)+':'+str(datetime.now().minute)\r\n",
    "    else:\r\n",
    "        if len(str(datetime.now().minute))==1:\r\n",
    "            time = str(datetime.now().hour)+':'+'0'+str(datetime.now().minute)\r\n",
    "        else:\r\n",
    "            time = str(datetime.now().hour)+':'+str(datetime.now().minute)\r\n",
    "    return \"\"\"\r\n",
    "        <!DOCTYPE html>\r\n",
    "<html lang=\"en\">\r\n",
    "\r\n",
    "<head>\r\n",
    "    <meta charset=\"UTF-8\">\r\n",
    "    <title>Chatbot</title>\r\n",
    "    <meta charset=\"UTF-8\">\r\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\r\n",
    "    <meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\">\r\n",
    "    <script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js\"></script>\r\n",
    "</head>\r\n",
    "<style>\r\n",
    ":root {\r\n",
    "    --body-bg: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);\r\n",
    "    --msger-bg: #fff;\r\n",
    "    --border: 2px solid #ddd;\r\n",
    "    --left-msg-bg: #ececec;\r\n",
    "    --right-msg-bg: #579ffb;\r\n",
    "  }\r\n",
    "  \r\n",
    "  html {\r\n",
    "    box-sizing: border-box;\r\n",
    "  }\r\n",
    "  \r\n",
    "  *,\r\n",
    "  *:before,\r\n",
    "  *:after {\r\n",
    "    margin: 0;\r\n",
    "    padding: 0;\r\n",
    "    box-sizing: inherit;\r\n",
    "  }\r\n",
    "  \r\n",
    "  body {\r\n",
    "    display: flex;\r\n",
    "    justify-content: center;\r\n",
    "    align-items: center;\r\n",
    "    height: 100vh;\r\n",
    "    background-image: var(--body-bg);\r\n",
    "    font-family: Helvetica, sans-serif;\r\n",
    "  }\r\n",
    "  \r\n",
    "  .msger {\r\n",
    "    display: flex;\r\n",
    "    flex-flow: column wrap;\r\n",
    "    justify-content: space-between;\r\n",
    "    width: 100%;\r\n",
    "    max-width: 867px;\r\n",
    "    margin: 25px 10px;\r\n",
    "    height: calc(100% - 50px);\r\n",
    "    border: var(--border);\r\n",
    "    border-radius: 5px;\r\n",
    "    background: var(--msger-bg);\r\n",
    "    box-shadow: 0 15px 15px -5px rgba(0, 0, 0, 0.2);\r\n",
    "  }\r\n",
    "  \r\n",
    "  .msger-header {\r\n",
    "    /* display: flex; */\r\n",
    "    font-size: medium;\r\n",
    "    justify-content: space-between;\r\n",
    "    padding: 10px;\r\n",
    "    text-align: center;\r\n",
    "    border-bottom: var(--border);\r\n",
    "    background: #eee;\r\n",
    "    color: #666;\r\n",
    "  }\r\n",
    "  \r\n",
    "  .msger-chat {\r\n",
    "    flex: 1;\r\n",
    "    overflow-y: auto;\r\n",
    "    padding: 10px;\r\n",
    "  }\r\n",
    "  .msger-chat::-webkit-scrollbar {\r\n",
    "    width: 6px;\r\n",
    "  }\r\n",
    "  .msger-chat::-webkit-scrollbar-track {\r\n",
    "    background: #ddd;\r\n",
    "  }\r\n",
    "  .msger-chat::-webkit-scrollbar-thumb {\r\n",
    "    background: #bdbdbd;\r\n",
    "  }\r\n",
    "  .msg {\r\n",
    "    display: flex;\r\n",
    "    align-items: flex-end;\r\n",
    "    margin-bottom: 10px;\r\n",
    "  }\r\n",
    "  \r\n",
    "  .msg-img {\r\n",
    "    width: 50px;\r\n",
    "    height: 50px;\r\n",
    "    margin-right: 10px;\r\n",
    "    background: #ddd;\r\n",
    "    background-repeat: no-repeat;\r\n",
    "    background-position: center;\r\n",
    "    background-size: cover;\r\n",
    "    border-radius: 50%;\r\n",
    "  }\r\n",
    "  .msg-bubble {\r\n",
    "    max-width: 450px;\r\n",
    "    padding: 15px;\r\n",
    "    border-radius: 15px;\r\n",
    "    background: var(--left-msg-bg);\r\n",
    "  }\r\n",
    "  .msg-info {\r\n",
    "    display: flex;\r\n",
    "    justify-content: space-between;\r\n",
    "    align-items: center;\r\n",
    "    margin-bottom: 10px;\r\n",
    "  }\r\n",
    "  .msg-info-name {\r\n",
    "    margin-right: 10px;\r\n",
    "    font-weight: bold;\r\n",
    "  }\r\n",
    "  .msg-info-time {\r\n",
    "    font-size: 0.85em;\r\n",
    "  }\r\n",
    "  \r\n",
    "  .left-msg .msg-bubble {\r\n",
    "    border-bottom-left-radius: 0;\r\n",
    "  }\r\n",
    "  \r\n",
    "  .right-msg {\r\n",
    "    flex-direction: row-reverse;\r\n",
    "  }\r\n",
    "  .right-msg .msg-bubble {\r\n",
    "    background: var(--right-msg-bg);\r\n",
    "    color: #fff;\r\n",
    "    border-bottom-right-radius: 0;\r\n",
    "  }\r\n",
    "  .right-msg .msg-img {\r\n",
    "    margin: 0 0 0 10px;\r\n",
    "  }\r\n",
    "  \r\n",
    "  .msger-inputarea {\r\n",
    "    display: flex;\r\n",
    "    padding: 10px;\r\n",
    "    border-top: var(--border);\r\n",
    "    background: #eee;\r\n",
    "  }\r\n",
    "  .msger-inputarea * {\r\n",
    "    padding: 10px;\r\n",
    "    border: none;\r\n",
    "    border-radius: 3px;\r\n",
    "    font-size: 1em;\r\n",
    "  }\r\n",
    "  .msger-input {\r\n",
    "    flex: 1;\r\n",
    "    background: #ddd;\r\n",
    "  }\r\n",
    "  .msger-send-btn {\r\n",
    "    margin-left: 10px;\r\n",
    "    background: rgb(0, 196, 65);\r\n",
    "    color: #fff;\r\n",
    "    font-weight: bold;\r\n",
    "    cursor: pointer;\r\n",
    "    transition: background 0.23s;\r\n",
    "  }\r\n",
    "  .msger-send-btn:hover {\r\n",
    "    background: rgb(0, 180, 50);\r\n",
    "  }\r\n",
    "  \r\n",
    "  .msger-chat {\r\n",
    "    background-color: #fcfcfe;\r\n",
    "  }\r\n",
    "</style>\r\n",
    "<body>\r\n",
    "    <section class=\"msger\">\r\n",
    "        <header class=\"msger-header\">\r\n",
    "            <div class=\"msger-header-title\">\r\n",
    "                Developer Chatbot using NLP in Cloab (Test)\r\n",
    "            </div>\r\n",
    "        </header>\r\n",
    "\r\n",
    "        <main class=\"msger-chat\">\r\n",
    "            <div class=\"msg left-msg\">\r\n",
    "                <div class=\"msg-img\" style=\"background-image: url(https://image.flaticon.com/icons/svg/327/327779.svg)\">\r\n",
    "                </div>\r\n",
    "\r\n",
    "                <div class=\"msg-bubble\">\r\n",
    "                    <div class=\"msg-info\">\r\n",
    "                        <div class=\"msg-info-name\">Chatbot</div>\r\n",
    "                    </div>\r\n",
    "\r\n",
    "                    <div class=\"msg-text\">\r\n",
    "                        안녕 애들아 모델 만들어보고 대화좀 해보고 여기서 성능 테스트 하면 될거같아\r\n",
    "                    </div>\r\n",
    "                </div>\r\n",
    "            </div>\r\n",
    "\r\n",
    "        </main>\r\n",
    "\r\n",
    "        <form class=\"msger-inputarea\">\r\n",
    "            <input type=\"text\" class=\"msger-input\" id=\"textInput\" placeholder=\"Enter your message...\">\r\n",
    "            <button type=\"submit\" class=\"msger-send-btn\">Send</button>\r\n",
    "        </form>\r\n",
    "    </section>\r\n",
    "    <script src='https://use.fontawesome.com/releases/v5.0.13/js/all.js'></script>\r\n",
    "    <script>\r\n",
    "\r\n",
    "        const msgerForm = get(\".msger-inputarea\");\r\n",
    "        const msgerInput = get(\".msger-input\");\r\n",
    "        const msgerChat = get(\".msger-chat\");\r\n",
    "\r\n",
    "        const BOT_IMG = \"https://image.flaticon.com/icons/svg/327/327779.svg\";\r\n",
    "        const PERSON_IMG = \"https://image.flaticon.com/icons/svg/145/145867.svg\";\r\n",
    "        const BOT_NAME = \"    ChatBot\";\r\n",
    "        const PERSON_NAME = \"You\";\r\n",
    "\r\n",
    "        msgerForm.addEventListener(\"submit\", event => {\r\n",
    "            event.preventDefault();\r\n",
    "\r\n",
    "            const msgText = msgerInput.value;\r\n",
    "            if (!msgText) return;\r\n",
    "\r\n",
    "            appendMessage(PERSON_NAME, PERSON_IMG, \"right\", msgText);\r\n",
    "            msgerInput.value = \"\";\r\n",
    "            botResponse(msgText);\r\n",
    "        });\r\n",
    "\r\n",
    "        function appendMessage(name, img, side, text) {\r\n",
    "            const msgHTML = `\r\n",
    "<div class=\"msg ${side}-msg\">\r\n",
    "  <div class=\"msg-img\" style=\"background-image: url(${img})\"></div>\r\n",
    "\r\n",
    "  <div class=\"msg-bubble\">\r\n",
    "    <div class=\"msg-info\">\r\n",
    "      <div class=\"msg-info-name\">${name}</div>\r\n",
    "      <div class=\"msg-info-time\">${formatDate(new Date())}</div>\r\n",
    "    </div>\r\n",
    "\r\n",
    "    <div class=\"msg-text\">${text}</div>\r\n",
    "  </div>\r\n",
    "</div>\r\n",
    "`;\r\n",
    "\r\n",
    "            msgerChat.insertAdjacentHTML(\"beforeend\", msgHTML);\r\n",
    "            msgerChat.scrollTop += 500;\r\n",
    "        }\r\n",
    "\r\n",
    "        function botResponse(rawText) {\r\n",
    "\r\n",
    "            $.get(\"/get\", { msg: rawText }).done(function (data) {\r\n",
    "                console.log(rawText);\r\n",
    "                console.log(data);\r\n",
    "                const msgText = data;\r\n",
    "                appendMessage(BOT_NAME, BOT_IMG, \"left\", msgText);\r\n",
    "\r\n",
    "            });\r\n",
    "\r\n",
    "        }\r\n",
    "\r\n",
    "        function get(selector, root = document) {\r\n",
    "            return root.querySelector(selector);\r\n",
    "        }\r\n",
    "\r\n",
    "        function formatDate(date) {\r\n",
    "            const h = \"0\" + date.getHours();\r\n",
    "            const m = \"0\" + date.getMinutes();\r\n",
    "\r\n",
    "            return `${h.slice(-2)}:${m.slice(-2)}`;\r\n",
    "        }\r\n",
    "\r\n",
    "    </script>\r\n",
    "\r\n",
    "</body>\r\n",
    "\r\n",
    "</html>\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "@app.route(\"/get\")\r\n",
    "def get_bot_response():\r\n",
    "    userText = request.args.get('msg')\r\n",
    "    return chatbot_response(userText)\r\n",
    "\r\n",
    "\r\n",
    "if __name__ == \"__main__\":\r\n",
    "    app.run()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\omw.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " * Serving Flask app '__main__' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [09/Sep/2021 18:29:16] \"GET / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "18\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "127.0.0.1 - - [09/Sep/2021 18:29:16] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [09/Sep/2021 18:29:22] \"GET /get?msg=hi HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Sep/2021 18:29:31] \"GET /get?msg=anyone%20can%20do%20this%3F HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " * Running on http://0dda-121-170-201-189.ngrok.io\n",
      " * Traffic stats available on http://127.0.0.1:4040\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "127.0.0.1 - - [09/Sep/2021 18:30:09] \"GET /get?msg=i%20have%20cold...%20what%20kind%20of%20drug%20i%20should%20have%3F HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Sep/2021 18:30:22] \"GET /get?msg=Navigating%20to%20what...%3F HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Sep/2021 18:30:39] \"GET /get?msg=give%20me%20the%20drug%20reaction%20list%20then HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('py38': conda)"
  },
  "interpreter": {
   "hash": "33a3111211be4281f3a8c4a9b25563b8d253df502c7e31f5318895c1792a97cb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}